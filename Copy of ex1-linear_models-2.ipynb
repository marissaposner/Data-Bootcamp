{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial demonstrates the basic usage in PyTorch. PyTorch is an open source machine learning library for python that have gained tremendous popularity due to a shallow learning curve, even for programmers with little experience in python. PyTorch uses several modules that makes it easy to define computational graphs, take gradients and optimaize neural networks. Currently, it is the fastest growing framework in terms of published papers in top conferences and industries alike.\n",
    "\n",
    "PyTorch is availble for all popular operating systems. It is recommended that you install it locally using [Conda](https://conda.io/docs/) (Python package manager) without GPU support, unless you know for sure that your system supports GPU acceleration and you know how to install Cuda and CudNN (Not recommended and not nessecary for this course). Make sure you install both `pytorch` and `torchvision` using the commands in the official [PyTorch](https://pytorch.org/) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch comes with several datasets ready for use in the torchvision package. This feature is highly usefull since obtaining and preprocessing datasets can be tedious and time consuming. The following commands will download the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset to your computer. Notice the parameters for the `DataLoader` function: we specify the root folder in which the dataset will be downloaded to. We use two different datasets - one for training and one for testing. Since neural networks can easily learn complex functions, we need to test the generalization capabilities of our network using data it have not seen before. Most mertics use the test data to measure how well the network perform. The `batch_size` parameter determines the number of images and their corresponding labels in each batch. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Transforms is a usefull library containing many operations on images.\n",
    "# Since the MNIST dataset is stored as PIL images, we need to transform it into tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_dataset_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(mnist_dataset_train, batch_size=4, shuffle=True, num_workers=2)\n",
    "mnist_dataset_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = torch.utils.data.DataLoader(mnist_dataset_test, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "classes = classes = [x for x in range(10)] # We use this notation for easy code recycling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple function to display the images in our dataset. Since PyTorch saves images in a channels first format, we need to rearrange the tensor using `np.transpose()` function. The following code snippet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:     8     7     1     3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "dataiter = iter(mnist_trainloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Ground Truth:',' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNIST_Net(nn.Module):\n",
    "    '''\n",
    "    Every network in PyTorch can (and should) be defined as a class. \n",
    "    Every class should have an init method containing the layers and\n",
    "    a forward method that defines how the layers are connected. \n",
    "    This is often called the architecture of the network. \n",
    "    PyTorch handles all backprogation automatically.\n",
    "    \n",
    "    Define the basic architecture of your network. For now, you should only use fully connected layers.\n",
    "    In PyTorch, fully connected layers are called Linear. You can read about them in :\n",
    "    https://pytorch.org/docs/stable/nn.html#linear-layers\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Define the basic architecture of your network. For now, only use fully    #\n",
    "        # connected layers. Read about calling fully connected layers at:           #\n",
    "        # https://pytorch.org/docs/stable/nn.html#linear-layers                     #\n",
    "        # In this function, you should only define the layers you intend to use.    #\n",
    "        # Save each layer as a different *self variable*.                           #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Define the forward propagation of your network. Connect each layer to     #\n",
    "        # the next and experiment with different activations, number of parameters  #\n",
    "        # and depths. You can read about different activations in PyTorch at        #\n",
    "        # https://pytorch.org/docs/stable/nn.html#non-linear-activation-functions   #\n",
    "        # Return a single tensor after passing it through your network.             #\n",
    "        # Hint: Shaping a multidimensional tensor into a vector can be achieved by: #\n",
    "        # the method x.view()                                                       #\n",
    "        #############################################################################\n",
    "        pass\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "mnist_net = MNIST_Net() # This line should instanciate your network as an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#############################################################################\n",
    "# TO DO:                                                                    #\n",
    "# Pick a loss function and optimizer from your network. Start with a cross- #\n",
    "# entropy loss and stochastic gradient descent with 0.001 learning rate and #\n",
    "# test the effect of different learning rates and momentum.                 #\n",
    "# Use the documentation and create variables to hold the loss function and  #\n",
    "# optimizer. The model will take them as inputs for the training process    #\n",
    "#############################################################################\n",
    "criterion = None\n",
    "optimizer = None\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, critertion, optimizer, trainloader):\n",
    "    #############################################################################\n",
    "    # TO DO:                                                                    #\n",
    "    # Train your network. Use the train loader to fetch a batch of data and     #\n",
    "    # labels. Then, zero the parameter gradients by using optimizer.zero_grad() #\n",
    "    # and perform a forward propagation and calculate the loss. Afterwards,     #\n",
    "    # calculate the gradients and backprob using loss.backward(), and perform   #\n",
    "    # the optimization step by using optimizer.step(). Use the provided         #\n",
    "    # statistics function to print useful information during training.          #\n",
    "    # Two iteration over the entire dataset (2 epochs) should be enough.        #\n",
    "    # Print the loss every 2000 batches to babysit the learning process.        #\n",
    "    #############################################################################\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_network(mnist_net, criterion, optimizer, mnist_trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works, the loss of the network should improved over time as it learns to classify hand-written digits.\n",
    "In order to measure how well the network performs, we need the test dataset.\n",
    "We classify each image in the test dataset (that the network never saw) and calculate the accuracy of the network. A good model should generalize and perform well even on data that was not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_net_accuracy(net, testloader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    \n",
    "calc_net_accuracy(mnist_net, mnist_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple network can achieve results of over 95%. Try several network architectures until you reach at least 93% accuracy.\n",
    "\n",
    "We can also take a batch and visualize the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(mnist_testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Ground Truth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = mnist_net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted   : ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the individual classification scores for each class in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_class_accuracy(net, testloader):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "        \n",
    "calc_class_accuracy(mnist_net, mnist_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented a simple neural network in PyTorch that predicts hand-written characters that scores over 95% in a matter of seconds. Next, we try to use the same network with more complicated data. This time, we will use [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). This dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "cifar_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar_trainloader = torch.utils.data.DataLoader(cifar_trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "cifar_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "cifar_testloader = torch.utils.data.DataLoader(cifar_testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "# get some random training images\n",
    "dataiter = iter(cifar_trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%10s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to use the same network we defined earlier. although both MNIST and CIFAR-10 output a vector of size 10, we need to adjust the input of the network, since MNIST images are 1x28x28 and CIFAR-10 images are 3x32x32. The optimizer and loss function are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR_Net(nn.Module):\n",
    "#############################################################################\n",
    "# TO DO:                                                                    #\n",
    "# Copy the architecture you used for the MNIST dataset. Change the size     #\n",
    "# of the input to support images from the CIFAR-10 dataset.                 #\n",
    "#############################################################################\n",
    "    pass\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "\n",
    "    \n",
    "cifar_net = CIFAR_Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cifar_net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(cifar_net, criterion, optimizer, cifar_trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results on the entire CIFAR-10 test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_net_accuracy(cifar_net, cifar_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(cifar_testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%7s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = cifar_net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted:   ', ' '.join('%7s' % classes[predicted[j]]for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_class_accuracy(cifar_net, cifar_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can predicting MNIST data with 95%+ accuracy, we are only able to predict less than half of the CIFAR-10 dataset using the same architecture. The reason is that CIFAR-10 data is more complicated, and we need a network that can better capture spatial invariant features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
