{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "* A replacement for NumPy to use the power of GPUs\n",
    "* A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "Much line Numpy, PyTorch provies an n-dimensional array object called a **Tensor** and a large variety of functions for manipulating these tensors. Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing and have built-in functionality that makes optimization (such as gradient descent) very easy to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "#torch a multi-dimensional matrix containing elements of a single data type.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.5846e+29, -4.0089e+25],\n",
      "        [-3.6902e+19,  8.3645e-21,  1.4013e-45],\n",
      "        [-1.2160e+23,  4.5915e-41, -1.2160e+23],\n",
      "        [ 4.5915e-41, -4.0063e+25,  8.5920e+09],\n",
      "        [-1.6480e-19,  4.5758e-41, -4.0093e+25]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "#creates empty torch\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1252, 0.2720, 0.9373],\n",
      "        [0.1027, 0.9548, 0.2476],\n",
      "        [0.5819, 0.4586, 0.9093],\n",
      "        [0.4750, 0.2189, 0.6960],\n",
      "        [0.2612, 0.7543, 0.9924]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to perform operations on tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9389, 0.5047, 1.0243],\n",
      "        [0.6411, 1.2122, 0.8252],\n",
      "        [0.9944, 0.8765, 0.9586],\n",
      "        [0.9934, 0.4333, 1.5537],\n",
      "        [0.8263, 1.0852, 1.5174]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9389, 0.5047, 1.0243],\n",
       "        [0.6411, 1.2122, 0.8252],\n",
       "        [0.9944, 0.8765, 0.9586],\n",
       "        [0.9934, 0.4333, 1.5537],\n",
       "        [0.8263, 1.0852, 1.5174]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9389, 0.5047, 1.0243],\n",
       "        [0.6411, 1.2122, 0.8252],\n",
       "        [0.9944, 0.8765, 0.9586],\n",
       "        [0.9934, 0.4333, 1.5537],\n",
       "        [0.8263, 1.0852, 1.5174]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to provide an output tensor as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9389, 0.5047, 1.0243],\n",
      "        [0.6411, 1.2122, 0.8252],\n",
      "        [0.9944, 0.8765, 0.9586],\n",
      "        [0.9934, 0.4333, 1.5537],\n",
      "        [0.8263, 1.0852, 1.5174]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or add in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9389, 0.5047, 1.0243],\n",
      "        [0.6411, 1.2122, 0.8252],\n",
      "        [0.9944, 0.8765, 0.9586],\n",
      "        [0.9934, 0.4333, 1.5537],\n",
      "        [0.8263, 1.0852, 1.5174]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping a tensor is done by `.view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "#view Returns a new tensor with the same data as the self tensor but of a different shape\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks. The **autograd** package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as True, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute. To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2., requires_grad=True)\n",
      "tensor(3., grad_fn=<AddBackward0>)\n",
      "tensor(-12., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(-2., requires_grad=True)\n",
    "y = torch.tensor( 5., requires_grad=True)\n",
    "z = torch.tensor(-4., requires_grad=True)\n",
    "q = x + y\n",
    "f = q * z\n",
    "print(x)\n",
    "print(q)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = x + y\n",
    "f = q * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: tensor(3., grad_fn=<AddBackward0>)\n",
      "q.grad: None\n",
      "f: tensor(-12., grad_fn=<MulBackward0>)\n",
      "f.grad: None\n"
     ]
    }
   ],
   "source": [
    "print('q:', q)\n",
    "print('q.grad:', q.grad) #.grad is the gradient attribute\n",
    "print('f:', f)\n",
    "print('f.grad:', f.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-15dabf95bc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x.grad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y.grad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z.grad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "f.backward() #started backpropogation from f and treated f as loss (gradient of a function with respect to itself is 1)\n",
    "print('x.grad', x.grad)\n",
    "print('y.grad', y.grad)\n",
    "print('z.grad', z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, user created Tensors have `requires_grad=False` so we can can't backprop through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad`` flag in-place. The input flag defaults to ``True`` if not given. In the following example, z contains enough information to compute gradients, as we saw above. Notice that if any input to an operation has ``requires_grad=True``, so will the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-af281ae8f349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grad:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "z = x + y\n",
    "print('z:', z)\n",
    "print('grad:', z.grad_fn)\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now z has the computation history that relates itself to x and y. We can take its values, and **detach** it from its history using `.detach()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_z = z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: None\n",
      "new_z: tensor([[4.0892, 1.0431],\n",
      "        [2.8883, 4.3756]])\n"
     ]
    }
   ],
   "source": [
    "print('grad:', new_z.grad_fn)\n",
    "print('new_z:', new_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How fast is PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30353083.882144533\n",
      "1 25668763.896842353\n",
      "2 23583361.138733905\n",
      "3 21057799.342128053\n",
      "4 17069447.82484387\n",
      "5 12387321.019240767\n",
      "6 8197281.786605861\n",
      "7 5174804.793345066\n",
      "8 3261709.5803906475\n",
      "9 2131523.587261985\n",
      "10 1472585.9472162933\n",
      "11 1079593.2228999073\n",
      "12 833596.6564089528\n",
      "13 669827.3146216973\n",
      "14 554126.9344169714\n",
      "15 467817.91719706735\n",
      "16 400673.1983593288\n",
      "17 346805.4460810479\n",
      "18 302449.15800454206\n",
      "19 265369.6513633472\n",
      "20 233935.64658472576\n",
      "21 207077.76303752972\n",
      "22 183956.31059123762\n",
      "23 163936.69564571802\n",
      "24 146517.44840085777\n",
      "25 131294.57832386103\n",
      "26 117945.84849572722\n",
      "27 106198.20006528826\n",
      "28 95830.42926634423\n",
      "29 86655.81830429999\n",
      "30 78510.94865287133\n",
      "31 71259.442994102\n",
      "32 64801.49153635087\n",
      "33 59034.85331533535\n",
      "34 53869.690661003384\n",
      "35 49232.24465630598\n",
      "36 45066.79081791034\n",
      "37 41312.513652883245\n",
      "38 37923.61128698618\n",
      "39 34857.92423308223\n",
      "40 32081.740583119867\n",
      "41 29562.800316174427\n",
      "42 27273.989794007484\n",
      "43 25191.06488990512\n",
      "44 23293.842003539638\n",
      "45 21565.028254220655\n",
      "46 19986.16146917338\n",
      "47 18540.99262470226\n",
      "48 17217.057606835493\n",
      "49 16002.639889640355\n",
      "50 14887.149823970118\n",
      "51 13861.64339099663\n",
      "52 12918.09089940619\n",
      "53 12048.623940012161\n",
      "54 11246.708185611502\n",
      "55 10506.107366969005\n",
      "56 9821.712350589898\n",
      "57 9188.59314309253\n",
      "58 8602.257138484896\n",
      "59 8058.7279533259125\n",
      "60 7554.539433872895\n",
      "61 7086.513780175216\n",
      "62 6651.670089280831\n",
      "63 6247.145846069035\n",
      "64 5870.733814416713\n",
      "65 5520.150332514077\n",
      "66 5193.566158187717\n",
      "67 4888.926173747715\n",
      "68 4604.530752246887\n",
      "69 4338.917918408152\n",
      "70 4090.6770470825427\n",
      "71 3858.51006763001\n",
      "72 3641.2189117897387\n",
      "73 3437.76137841352\n",
      "74 3247.0946612698176\n",
      "75 3068.3533710331817\n",
      "76 2900.7406866608862\n",
      "77 2743.4103247073726\n",
      "78 2595.61995132195\n",
      "79 2456.8503458909263\n",
      "80 2326.356725308957\n",
      "81 2203.5716019258402\n",
      "82 2088.0191547242757\n",
      "83 1979.2236933126349\n",
      "84 1876.7708272456882\n",
      "85 1780.2041594186567\n",
      "86 1689.1541731998964\n",
      "87 1603.2667690887583\n",
      "88 1522.230219024044\n",
      "89 1445.7026501780413\n",
      "90 1373.4194300592026\n",
      "91 1305.1553231974585\n",
      "92 1240.6306576620386\n",
      "93 1179.6334232076993\n",
      "94 1121.9369982998376\n",
      "95 1067.3437462211873\n",
      "96 1015.6945133738293\n",
      "97 966.7743718443726\n",
      "98 920.4314106004806\n",
      "99 876.5208305777383\n",
      "100 834.9367598734794\n",
      "101 795.5064674990351\n",
      "102 758.0930354940057\n",
      "103 722.6092672452166\n",
      "104 688.9307646751754\n",
      "105 656.9533819227754\n",
      "106 626.5959056801969\n",
      "107 597.7641592801951\n",
      "108 570.3725187515316\n",
      "109 544.3410011510189\n",
      "110 519.5959360749588\n",
      "111 496.0796808223353\n",
      "112 473.7046878044597\n",
      "113 452.4219129237672\n",
      "114 432.17696161638423\n",
      "115 412.911338869756\n",
      "116 394.5645329431767\n",
      "117 377.09693326970955\n",
      "118 360.46510338402163\n",
      "119 344.6296258405232\n",
      "120 329.5302755705119\n",
      "121 315.1431260245522\n",
      "122 301.42884107225734\n",
      "123 288.35449734443296\n",
      "124 275.8870096173726\n",
      "125 263.9968906623881\n",
      "126 252.65352791449965\n",
      "127 241.83213961533733\n",
      "128 231.504101494123\n",
      "129 221.64528846947945\n",
      "130 212.2343507543855\n",
      "131 203.25137759627535\n",
      "132 194.66948284947412\n",
      "133 186.4729534738678\n",
      "134 178.64649657469445\n",
      "135 171.1702110252235\n",
      "136 164.02023208127167\n",
      "137 157.189511288556\n",
      "138 150.65904404057375\n",
      "139 144.41466430870844\n",
      "140 138.4454710681922\n",
      "141 132.73568302776093\n",
      "142 127.27443679993684\n",
      "143 122.05135188357468\n",
      "144 117.05318530188572\n",
      "145 112.27045737067074\n",
      "146 107.6950738957608\n",
      "147 103.3150454657306\n",
      "148 99.12293623271216\n",
      "149 95.11223481929018\n",
      "150 91.26942362321006\n",
      "151 87.58921326096628\n",
      "152 84.06436551802008\n",
      "153 80.68809925898512\n",
      "154 77.45437198524678\n",
      "155 74.3557701177416\n",
      "156 71.38686268829596\n",
      "157 68.5424795165633\n",
      "158 65.8160555920312\n",
      "159 63.203148160841316\n",
      "160 60.698288226109945\n",
      "161 58.29756723891036\n",
      "162 55.9973152002061\n",
      "163 53.789501566169825\n",
      "164 51.673135370887024\n",
      "165 49.64309094352924\n",
      "166 47.69557225508596\n",
      "167 45.82810976170096\n",
      "168 44.036091253454956\n",
      "169 42.31726908523563\n",
      "170 40.66798408055319\n",
      "171 39.08523301386184\n",
      "172 37.5668931619402\n",
      "173 36.109428118296165\n",
      "174 34.71153226413401\n",
      "175 33.368565000874035\n",
      "176 32.07952092359693\n",
      "177 30.842030743778487\n",
      "178 29.653653568059816\n",
      "179 28.512939366198523\n",
      "180 27.4172472440129\n",
      "181 26.365298507722606\n",
      "182 25.35506736194693\n",
      "183 24.38476767076179\n",
      "184 23.45281315045896\n",
      "185 22.557825280986016\n",
      "186 21.69792055031656\n",
      "187 20.87143714579218\n",
      "188 20.077573858601163\n",
      "189 19.314654008364464\n",
      "190 18.58163933427418\n",
      "191 17.877212874083472\n",
      "192 17.200236078281108\n",
      "193 16.549685136139193\n",
      "194 15.924367966066205\n",
      "195 15.323517421634536\n",
      "196 14.746058466613526\n",
      "197 14.190649742060883\n",
      "198 13.65669150955989\n",
      "199 13.143410026602947\n",
      "200 12.649850766107846\n",
      "201 12.175370221218397\n",
      "202 11.719007903995166\n",
      "203 11.280307569700401\n",
      "204 10.858319975262637\n",
      "205 10.45257848343402\n",
      "206 10.062584851920636\n",
      "207 9.687207943574649\n",
      "208 9.326132068733507\n",
      "209 8.97888513702241\n",
      "210 8.644804632153669\n",
      "211 8.323476082450945\n",
      "212 8.014281973249961\n",
      "213 7.716880320381077\n",
      "214 7.430711069404648\n",
      "215 7.155409633726266\n",
      "216 6.890683947340619\n",
      "217 6.635800998446751\n",
      "218 6.390551677717267\n",
      "219 6.1545376470355215\n",
      "220 5.92740140006457\n",
      "221 5.708805283220676\n",
      "222 5.498433292736548\n",
      "223 5.295940858603614\n",
      "224 5.101079758977029\n",
      "225 4.913555047367247\n",
      "226 4.733067847138349\n",
      "227 4.559289992154488\n",
      "228 4.391990490615504\n",
      "229 4.230950951540197\n",
      "230 4.075897590007683\n",
      "231 3.926652978903979\n",
      "232 3.782964615572763\n",
      "233 3.6445958657064472\n",
      "234 3.511425267688114\n",
      "235 3.3832153001147587\n",
      "236 3.2597160679743133\n",
      "237 3.1408323439876504\n",
      "238 3.0263302690604403\n",
      "239 2.9160776158663895\n",
      "240 2.8098974398228975\n",
      "241 2.707633350763282\n",
      "242 2.6091669265276876\n",
      "243 2.514330201395478\n",
      "244 2.4230321352702626\n",
      "245 2.3350613758360077\n",
      "246 2.2503265474117113\n",
      "247 2.168726453482997\n",
      "248 2.0901166779682665\n",
      "249 2.014409913475889\n",
      "250 1.941483261421908\n",
      "251 1.871219475793362\n",
      "252 1.8035506902892529\n",
      "253 1.7383794390210836\n",
      "254 1.6755652057396255\n",
      "255 1.615070178966055\n",
      "256 1.5567813530067167\n",
      "257 1.5006184574472585\n",
      "258 1.4465157939978868\n",
      "259 1.3943852357951245\n",
      "260 1.3441646970972818\n",
      "261 1.2957831674369213\n",
      "262 1.2491555416962303\n",
      "263 1.2042324055019806\n",
      "264 1.1609366983489866\n",
      "265 1.1192170287712062\n",
      "266 1.079020679517797\n",
      "267 1.0402780178290847\n",
      "268 1.0029485087674364\n",
      "269 0.9669784493454044\n",
      "270 0.9323220010051072\n",
      "271 0.8989149472957163\n",
      "272 0.866716946788102\n",
      "273 0.8356864205301888\n",
      "274 0.8057823155029302\n",
      "275 0.7769534087194295\n",
      "276 0.7491714136053002\n",
      "277 0.7223936575022307\n",
      "278 0.6965907566075364\n",
      "279 0.6717174425407632\n",
      "280 0.6477387274598609\n",
      "281 0.6246262301571426\n",
      "282 0.6023492036723119\n",
      "283 0.580870374262815\n",
      "284 0.5601687734912465\n",
      "285 0.5402135432124344\n",
      "286 0.5209791184287054\n",
      "287 0.5024361034278314\n",
      "288 0.48455726186326487\n",
      "289 0.4673211028129638\n",
      "290 0.4507069665973322\n",
      "291 0.4346906447061379\n",
      "292 0.41924710196390347\n",
      "293 0.404357991770363\n",
      "294 0.3900048546448638\n",
      "295 0.37616565049608913\n",
      "296 0.36282111066733924\n",
      "297 0.3499532958262056\n",
      "298 0.3375505139523312\n",
      "299 0.3255873418655856\n",
      "300 0.31405323607038493\n",
      "301 0.3029347684654266\n",
      "302 0.2922136133273361\n",
      "303 0.2818738385360945\n",
      "304 0.27190335354149764\n",
      "305 0.26228747593932256\n",
      "306 0.25301665379163873\n",
      "307 0.24407607440774806\n",
      "308 0.23545365788647635\n",
      "309 0.22714102779941228\n",
      "310 0.2191241234097964\n",
      "311 0.21139218671985024\n",
      "312 0.20393597920610157\n",
      "313 0.19674337009538176\n",
      "314 0.18980814260132123\n",
      "315 0.1831188359337772\n",
      "316 0.17666712453991318\n",
      "317 0.17044668723922168\n",
      "318 0.16444663811860447\n",
      "319 0.15865842541443737\n",
      "320 0.1530766640040948\n",
      "321 0.14769182886588123\n",
      "322 0.14249954951287463\n",
      "323 0.13749057427995942\n",
      "324 0.13265867620458002\n",
      "325 0.12799983392164568\n",
      "326 0.1235055954911598\n",
      "327 0.11916939379077503\n",
      "328 0.11498737005507811\n",
      "329 0.11095242026544014\n",
      "330 0.1070610090602824\n",
      "331 0.10330713019567125\n",
      "332 0.0996857896257405\n",
      "333 0.09619359287546181\n",
      "334 0.09282377296059455\n",
      "335 0.08957258872829245\n",
      "336 0.08643706576465399\n",
      "337 0.08341121664058418\n",
      "338 0.08049240282349905\n",
      "339 0.07767705928863179\n",
      "340 0.07496039303468952\n",
      "341 0.07234029021328976\n",
      "342 0.06981224556684783\n",
      "343 0.0673730525485312\n",
      "344 0.06502010540021451\n",
      "345 0.0627493951520425\n",
      "346 0.06055865201108091\n",
      "347 0.05844530429169045\n",
      "348 0.05640600009765824\n",
      "349 0.05443870307471514\n",
      "350 0.05254041379100563\n",
      "351 0.05070844997730048\n",
      "352 0.048941511971297876\n",
      "353 0.047236260112517775\n",
      "354 0.04559070879846633\n",
      "355 0.04400333129563672\n",
      "356 0.0424715011281547\n",
      "357 0.04099317334523231\n",
      "358 0.03956688800993603\n",
      "359 0.03819043463196094\n",
      "360 0.03686246228262687\n",
      "361 0.035580807682402626\n",
      "362 0.0343440832763771\n",
      "363 0.03315099334588195\n",
      "364 0.03199936589816297\n",
      "365 0.030887879593051684\n",
      "366 0.029815514495734514\n",
      "367 0.028780333964318016\n",
      "368 0.02778154417643095\n",
      "369 0.02681772038838283\n",
      "370 0.025887350446659678\n",
      "371 0.024989870061155144\n",
      "372 0.024123587243717222\n",
      "373 0.023287392010317643\n",
      "374 0.022480631796284685\n",
      "375 0.021701747304932774\n",
      "376 0.020950191784862127\n",
      "377 0.02022481909052214\n",
      "378 0.019524665873219573\n",
      "379 0.018849092058909275\n",
      "380 0.018196910201360186\n",
      "381 0.01756735600273336\n",
      "382 0.016960043626082354\n",
      "383 0.01637360470106987\n",
      "384 0.015807623251870268\n",
      "385 0.015261437007971368\n",
      "386 0.014734165040735641\n",
      "387 0.014225236898123484\n",
      "388 0.013734023038832383\n",
      "389 0.013259795510251991\n",
      "390 0.012802178226165679\n",
      "391 0.012360381837789414\n",
      "392 0.011934044583592845\n",
      "393 0.011522510598621138\n",
      "394 0.011125160545811818\n",
      "395 0.010741627704126278\n",
      "396 0.010371401783368282\n",
      "397 0.010013952449187595\n",
      "398 0.00966901327228681\n",
      "399 0.009335970301328447\n",
      "400 0.009014443403263676\n",
      "401 0.008704204569198246\n",
      "402 0.008404609920129834\n",
      "403 0.008115384937326945\n",
      "404 0.00783620959894795\n",
      "405 0.007566645338543801\n",
      "406 0.0073064609272538075\n",
      "407 0.007055266547069935\n",
      "408 0.0068127648250189635\n",
      "409 0.006578689897371998\n",
      "410 0.006352620979305346\n",
      "411 0.006134424183063525\n",
      "412 0.00592379014026667\n",
      "413 0.005720365050579979\n",
      "414 0.005524016531828503\n",
      "415 0.0053344537147359625\n",
      "416 0.0051513938979792255\n",
      "417 0.004974708628046259\n",
      "418 0.004804074330269977\n",
      "419 0.0046393374745598405\n",
      "420 0.004480300082003201\n",
      "421 0.004326737766318624\n",
      "422 0.004178498446806631\n",
      "423 0.0040353557835326554\n",
      "424 0.003897111334120107\n",
      "425 0.003763680990456327\n",
      "426 0.003634799178984235\n",
      "427 0.003510366628571549\n",
      "428 0.0033902394380367733\n",
      "429 0.0032742138054628637\n",
      "430 0.003162220754812901\n",
      "431 0.0030540830177589\n",
      "432 0.002949628710604001\n",
      "433 0.0028488018859207467\n",
      "434 0.0027514087278989238\n",
      "435 0.0026573774035121432\n",
      "436 0.002566590967252887\n",
      "437 0.0024789122806857396\n",
      "438 0.002394268644118677\n",
      "439 0.0023125051417689393\n",
      "440 0.0022335509334316423\n",
      "441 0.002157337381694198\n",
      "442 0.0020837011242654186\n",
      "443 0.002012602273267863\n",
      "444 0.0019439515080488525\n",
      "445 0.001877642503882059\n",
      "446 0.0018136251452015258\n",
      "447 0.0017517906607860782\n",
      "448 0.0016920738556716421\n",
      "449 0.001634420282231252\n",
      "450 0.0015787309474524689\n",
      "451 0.0015249549095849762\n",
      "452 0.001473017804838211\n",
      "453 0.001422853057198489\n",
      "454 0.0013744177134731093\n",
      "455 0.0013276301135605748\n",
      "456 0.001282441998805668\n",
      "457 0.0012388111553306117\n",
      "458 0.0011966610775332907\n",
      "459 0.0011559648457270338\n",
      "460 0.0011166583710599993\n",
      "461 0.0010786867186772374\n",
      "462 0.001042022826943526\n",
      "463 0.0010066014122183339\n",
      "464 0.0009723934760182635\n",
      "465 0.0009393618866228827\n",
      "466 0.0009074464279198362\n",
      "467 0.0008766280121399062\n",
      "468 0.0008468633530156574\n",
      "469 0.000818113058488321\n",
      "470 0.000790350051165506\n",
      "471 0.0007635229464271591\n",
      "472 0.0007376137599868228\n",
      "473 0.0007125909401394688\n",
      "474 0.0006884177220514143\n",
      "475 0.0006650744105100091\n",
      "476 0.0006425224368955982\n",
      "477 0.0006207369177770157\n",
      "478 0.0005997054247847711\n",
      "479 0.0005793791782609108\n",
      "480 0.0005597481655082131\n",
      "481 0.0005407868131656933\n",
      "482 0.0005224665459136752\n",
      "483 0.0005047761630993074\n",
      "484 0.00048768307810777756\n",
      "485 0.0004711728612146813\n",
      "486 0.0004552262674331403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487 0.0004398186946544849\n",
      "488 0.00042494076070403417\n",
      "489 0.00041056371357877454\n",
      "490 0.00039667449643360187\n",
      "491 0.0003832635481721061\n",
      "492 0.0003703030642206656\n",
      "493 0.00035778500295288566\n",
      "494 0.00034569171704425304\n",
      "495 0.0003340067108058794\n",
      "496 0.00032272280181512217\n",
      "497 0.0003118205204922757\n",
      "498 0.0003012870908658721\n",
      "499 0.00029111303443993936\n",
      "CPU times: user 1.48 s, sys: 144 ms, total: 1.62 s\n",
      "Wall time: 860 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3506759424.0\n",
      "1 3442005760.0\n",
      "2 3872572416.0\n",
      "3 4842099712.0\n",
      "4 6466561024.0\n",
      "5 8939028480.0\n",
      "6 12538347520.0\n",
      "7 17632937984.0\n",
      "8 24666298368.0\n",
      "9 34092261376.0\n",
      "10 46237696000.0\n",
      "11 61018857472.0\n",
      "12 77556744192.0\n",
      "13 93758996480.0\n",
      "14 106339188736.0\n",
      "15 111537528832.0\n",
      "16 106999906304.0\n",
      "17 93297213440.0\n",
      "18 74130939904.0\n",
      "19 54218817536.0\n",
      "20 37093236736.0\n",
      "21 24143513600.0\n",
      "22 15186069504.0\n",
      "23 9338962944.0\n",
      "24 5663052800.0\n",
      "25 3404229120.0\n",
      "26 2035833088.0\n",
      "27 1213700224.0\n",
      "28 722291840.0\n",
      "29 429404640.0\n",
      "30 255148048.0\n",
      "31 151566720.0\n",
      "32 90030200.0\n",
      "33 53479856.0\n",
      "34 31772468.0\n",
      "35 18879692.0\n",
      "36 11221504.0\n",
      "37 6671803.5\n",
      "38 3968286.25\n",
      "39 2361371.25\n",
      "40 1405951.0\n",
      "41 837670.8125\n",
      "42 499506.90625\n",
      "43 298165.96875\n",
      "44 178207.25\n",
      "45 106678.7734375\n",
      "46 63985.5078125\n",
      "47 38471.21875\n",
      "48 23200.50390625\n",
      "49 14043.7509765625\n",
      "50 8540.0751953125\n",
      "51 5222.67041015625\n",
      "52 3215.85693359375\n",
      "53 1996.4759521484375\n",
      "54 1251.5301513671875\n",
      "55 793.4547119140625\n",
      "56 509.51483154296875\n",
      "57 331.8939208984375\n",
      "58 219.54974365234375\n",
      "59 147.59805297851562\n",
      "60 100.89339447021484\n",
      "61 70.10511779785156\n",
      "62 49.496131896972656\n",
      "63 35.48572540283203\n",
      "64 25.80438232421875\n",
      "65 19.00537872314453\n",
      "66 14.181964874267578\n",
      "67 10.714775085449219\n",
      "68 8.19487190246582\n",
      "69 6.33785343170166\n",
      "70 4.9564056396484375\n",
      "71 3.9210546016693115\n",
      "72 3.137101173400879\n",
      "73 2.5332934856414795\n",
      "74 2.06687331199646\n",
      "75 1.6991556882858276\n",
      "76 1.4087523221969604\n",
      "77 1.1764614582061768\n",
      "78 0.988744854927063\n",
      "79 0.8348225951194763\n",
      "80 0.7105016112327576\n",
      "81 0.6068503856658936\n",
      "82 0.5207798480987549\n",
      "83 0.44983407855033875\n",
      "84 0.3898695409297943\n",
      "85 0.33894824981689453\n",
      "86 0.29678332805633545\n",
      "87 0.2601957619190216\n",
      "88 0.22906795144081116\n",
      "89 0.20265118777751923\n",
      "90 0.17976579070091248\n",
      "91 0.1596774160861969\n",
      "92 0.14222854375839233\n",
      "93 0.12710395455360413\n",
      "94 0.1145547479391098\n",
      "95 0.10332141816616058\n",
      "96 0.09310725331306458\n",
      "97 0.08423784375190735\n",
      "98 0.07694441080093384\n",
      "99 0.06993965059518814\n",
      "100 0.06413628906011581\n",
      "101 0.058842748403549194\n",
      "102 0.05402221903204918\n",
      "103 0.04996977746486664\n",
      "104 0.046224772930145264\n",
      "105 0.04271045699715614\n",
      "106 0.03960172459483147\n",
      "107 0.03677511587738991\n",
      "108 0.03441917523741722\n",
      "109 0.032147474586963654\n",
      "110 0.029984064400196075\n",
      "111 0.028150148689746857\n",
      "112 0.026452230289578438\n",
      "113 0.024985194206237793\n",
      "114 0.023402372375130653\n",
      "115 0.022137461230158806\n",
      "116 0.020873364061117172\n",
      "117 0.019694464281201363\n",
      "118 0.018748309463262558\n",
      "119 0.01773221418261528\n",
      "120 0.016926877200603485\n",
      "121 0.01605718582868576\n",
      "122 0.015266971662640572\n",
      "123 0.014597773551940918\n",
      "124 0.013926081359386444\n",
      "125 0.013315265066921711\n",
      "126 0.012724802829325199\n",
      "127 0.012238023802638054\n",
      "128 0.011688964441418648\n",
      "129 0.011252508498728275\n",
      "130 0.010796009562909603\n",
      "131 0.010330956429243088\n",
      "132 0.009893860667943954\n",
      "133 0.009499667212367058\n",
      "134 0.009198298677802086\n",
      "135 0.008825244382023811\n",
      "136 0.00850536860525608\n",
      "137 0.008213646709918976\n",
      "138 0.007945859804749489\n",
      "139 0.007639995776116848\n",
      "140 0.007471797056496143\n",
      "141 0.0071758972480893135\n",
      "142 0.006943588610738516\n",
      "143 0.006727010477334261\n",
      "144 0.006506585981696844\n",
      "145 0.006334632635116577\n",
      "146 0.006120099686086178\n",
      "147 0.005955268628895283\n",
      "148 0.005760830361396074\n",
      "149 0.005589071661233902\n",
      "150 0.005457248538732529\n",
      "151 0.005293616559356451\n",
      "152 0.00512939877808094\n",
      "153 0.004998171702027321\n",
      "154 0.004870826844125986\n",
      "155 0.0047460575588047504\n",
      "156 0.004560383968055248\n",
      "157 0.004489703103899956\n",
      "158 0.004382437560707331\n",
      "159 0.004266473930329084\n",
      "160 0.0041745929047465324\n",
      "161 0.004060559906065464\n",
      "162 0.003955185413360596\n",
      "163 0.003864530473947525\n",
      "164 0.0037540558259934187\n",
      "165 0.003664039308205247\n",
      "166 0.003566281171515584\n",
      "167 0.0035158004611730576\n",
      "168 0.0034311339259147644\n",
      "169 0.003353205043822527\n",
      "170 0.00326581671833992\n",
      "171 0.003175680059939623\n",
      "172 0.0031136497855186462\n",
      "173 0.0030313448514789343\n",
      "174 0.0029730643145740032\n",
      "175 0.002919173799455166\n",
      "176 0.0028657361399382353\n",
      "177 0.002805746626108885\n",
      "178 0.0027498886920511723\n",
      "179 0.002683863043785095\n",
      "180 0.0026271548122167587\n",
      "181 0.0025773122906684875\n",
      "182 0.0025329929776489735\n",
      "183 0.002475935034453869\n",
      "184 0.0024235625751316547\n",
      "185 0.0023878312204033136\n",
      "186 0.002360708313062787\n",
      "187 0.002303977031260729\n",
      "188 0.0022660845424979925\n",
      "189 0.0022170385345816612\n",
      "190 0.0021725893020629883\n",
      "191 0.0021266178227961063\n",
      "192 0.0021006804890930653\n",
      "193 0.00206396309658885\n",
      "194 0.002037878381088376\n",
      "195 0.002006406895816326\n",
      "196 0.001985362730920315\n",
      "197 0.0019411620451137424\n",
      "198 0.0019126415718346834\n",
      "199 0.0018812073394656181\n",
      "200 0.0018548892112448812\n",
      "201 0.0018248130800202489\n",
      "202 0.0017769321566447616\n",
      "203 0.0017289463430643082\n",
      "204 0.0016969343414530158\n",
      "205 0.0016821310855448246\n",
      "206 0.001646593795157969\n",
      "207 0.001627958146855235\n",
      "208 0.0016140058869495988\n",
      "209 0.0015902691520750523\n",
      "210 0.0015646768733859062\n",
      "211 0.0015405029989778996\n",
      "212 0.00150200049392879\n",
      "213 0.0014810683205723763\n",
      "214 0.0014632284874096513\n",
      "215 0.0014355508610606194\n",
      "216 0.001422440866008401\n",
      "217 0.0014092993224039674\n",
      "218 0.001387248164974153\n",
      "219 0.001367368851788342\n",
      "220 0.0013521566288545728\n",
      "221 0.00134351069573313\n",
      "222 0.0013235431397333741\n",
      "223 0.0012998252641409636\n",
      "224 0.0012878708075731993\n",
      "225 0.00126253766939044\n",
      "226 0.0012446660548448563\n",
      "227 0.001232277019880712\n",
      "228 0.0012125364737585187\n",
      "229 0.0012014975072816014\n",
      "230 0.0011777197942137718\n",
      "231 0.0011643951293081045\n",
      "232 0.0011489811586216092\n",
      "233 0.0011340414639562368\n",
      "234 0.0011268486268818378\n",
      "235 0.001113271340727806\n",
      "236 0.0011006987188011408\n",
      "237 0.001089534955099225\n",
      "238 0.0010776725830510259\n",
      "239 0.0010567064164206386\n",
      "240 0.001047287485562265\n",
      "241 0.001041889889165759\n",
      "242 0.0010287777986377478\n",
      "243 0.0010241825366392732\n",
      "244 0.001006188103929162\n",
      "245 0.0009998506866395473\n",
      "246 0.0009907904313877225\n",
      "247 0.0009740020032040775\n",
      "248 0.0009680049261078238\n",
      "249 0.0009448723867535591\n",
      "250 0.0009355422807857394\n",
      "251 0.0009286652784794569\n",
      "252 0.0009201973443850875\n",
      "253 0.00091101776342839\n",
      "254 0.0008977664401754737\n",
      "255 0.000885424145963043\n",
      "256 0.0008627957431599498\n",
      "257 0.0008539760019630194\n",
      "258 0.0008491710177622736\n",
      "259 0.0008458310039713979\n",
      "260 0.0008382610976696014\n",
      "261 0.0008245226927101612\n",
      "262 0.0008140966529026628\n",
      "263 0.0008025355054996908\n",
      "264 0.000792416394688189\n",
      "265 0.0007921101641841233\n",
      "266 0.000782641232945025\n",
      "267 0.0007746806368231773\n",
      "268 0.0007581515819765627\n",
      "269 0.0007519451319240034\n",
      "270 0.0007419821922667325\n",
      "271 0.0007375996792688966\n",
      "272 0.000732176995370537\n",
      "273 0.0007315319962799549\n",
      "274 0.0007196301594376564\n",
      "275 0.0007112666498869658\n",
      "276 0.000705645012203604\n",
      "277 0.0006987760425545275\n",
      "278 0.0006872166413813829\n",
      "279 0.0006866247858852148\n",
      "280 0.0006800303817726672\n",
      "281 0.0006731557659804821\n",
      "282 0.0006652286974713206\n",
      "283 0.0006545686046592891\n",
      "284 0.0006478485884144902\n",
      "285 0.0006416401593014598\n",
      "286 0.0006337084341794252\n",
      "287 0.0006263017421588302\n",
      "288 0.0006228210404515266\n",
      "289 0.0006153773283585906\n",
      "290 0.0006108162924647331\n",
      "291 0.0006080697057768703\n",
      "292 0.000601526815444231\n",
      "293 0.0005923093995079398\n",
      "294 0.0005864629056304693\n",
      "295 0.0005819397047162056\n",
      "296 0.0005774704040959477\n",
      "297 0.0005692591657862067\n",
      "298 0.0005677658482454717\n",
      "299 0.0005660068127326667\n",
      "300 0.0005559095880016685\n",
      "301 0.0005524929147213697\n",
      "302 0.0005499802646227181\n",
      "303 0.0005469274474307895\n",
      "304 0.0005458557279780507\n",
      "305 0.0005438457010313869\n",
      "306 0.0005369707942008972\n",
      "307 0.0005308001418597996\n",
      "308 0.00052691251039505\n",
      "309 0.0005222936160862446\n",
      "310 0.0005188089562579989\n",
      "311 0.0005142741138115525\n",
      "312 0.0005121738649904728\n",
      "313 0.0005119572160765529\n",
      "314 0.0005035952781327069\n",
      "315 0.0005010266322642565\n",
      "316 0.0004989727749489248\n",
      "317 0.000491296174004674\n",
      "318 0.0004885293310508132\n",
      "319 0.00048376061022281647\n",
      "320 0.0004799210873898119\n",
      "321 0.00047947349958121777\n",
      "322 0.00047583013656549156\n",
      "323 0.00047268380876630545\n",
      "324 0.00046818359987810254\n",
      "325 0.00046400525025092065\n",
      "326 0.0004578590742312372\n",
      "327 0.0004542170208878815\n",
      "328 0.0004541841335594654\n",
      "329 0.00045059554395265877\n",
      "330 0.00044628616888076067\n",
      "331 0.0004435188602656126\n",
      "332 0.000443083728896454\n",
      "333 0.00043714072671718895\n",
      "334 0.00043548221583478153\n",
      "335 0.0004360744496807456\n",
      "336 0.00043373589869588614\n",
      "337 0.0004292217781767249\n",
      "338 0.0004270962381269783\n",
      "339 0.0004246503231115639\n",
      "340 0.00042365759145468473\n",
      "341 0.0004197459202259779\n",
      "342 0.00041691173100844026\n",
      "343 0.00041208084439858794\n",
      "344 0.00040575850289314985\n",
      "345 0.0004007414390798658\n",
      "346 0.00039824063424021006\n",
      "347 0.00039352537714876235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 0.00039149820804595947\n",
      "349 0.00038504175608977675\n",
      "350 0.00038161143311299384\n",
      "351 0.000377550779376179\n",
      "352 0.00037544724182225764\n",
      "353 0.00037146260729059577\n",
      "354 0.0003692488535307348\n",
      "355 0.00036583031760528684\n",
      "356 0.0003626970574259758\n",
      "357 0.0003593633300624788\n",
      "358 0.00035309421946294606\n",
      "359 0.00035290338564664125\n",
      "360 0.00035277236020192504\n",
      "361 0.0003502930048853159\n",
      "362 0.00034667967702262104\n",
      "363 0.00034799915738403797\n",
      "364 0.0003457299608271569\n",
      "365 0.0003437247942201793\n",
      "366 0.0003392737708054483\n",
      "367 0.00033849518513306975\n",
      "368 0.0003338655806146562\n",
      "369 0.00033152959076687694\n",
      "370 0.00032909552101045847\n",
      "371 0.0003283742698840797\n",
      "372 0.0003269539156462997\n",
      "373 0.00032723587355576456\n",
      "374 0.0003272026660852134\n",
      "375 0.0003265567938797176\n",
      "376 0.0003231565351597965\n",
      "377 0.00032313517294824123\n",
      "378 0.000321834726491943\n",
      "379 0.0003195242607034743\n",
      "380 0.0003156077000312507\n",
      "381 0.00031574309105053544\n",
      "382 0.00031426240457221866\n",
      "383 0.00031276760273613036\n",
      "384 0.0003094331477768719\n",
      "385 0.00030392457847483456\n",
      "386 0.000302933098282665\n",
      "387 0.00029931956669315696\n",
      "388 0.00029866400291211903\n",
      "389 0.0002937380049843341\n",
      "390 0.00029291323153302073\n",
      "391 0.00028918037423864007\n",
      "392 0.00028618861688300967\n",
      "393 0.00028698527603410184\n",
      "394 0.00028599938377738\n",
      "395 0.000283936969935894\n",
      "396 0.00028235308127477765\n",
      "397 0.000281439715763554\n",
      "398 0.00028061040211468935\n",
      "399 0.00028009986272081733\n",
      "400 0.00027838684036396444\n",
      "401 0.00027593140839599073\n",
      "402 0.00027291447622701526\n",
      "403 0.0002708059619180858\n",
      "404 0.00027064632740803063\n",
      "405 0.0002687934320420027\n",
      "406 0.00026880245422944427\n",
      "407 0.0002676751755643636\n",
      "408 0.0002660463214851916\n",
      "409 0.0002654636336956173\n",
      "410 0.0002640408056322485\n",
      "411 0.0002632407995406538\n",
      "412 0.0002623131440486759\n",
      "413 0.0002616482088342309\n",
      "414 0.0002573010860942304\n",
      "415 0.0002555906539782882\n",
      "416 0.00025442277546972036\n",
      "417 0.00025307893520221114\n",
      "418 0.0002532675862312317\n",
      "419 0.00025146512780338526\n",
      "420 0.00025084574008360505\n",
      "421 0.0002490420010872185\n",
      "422 0.0002465508587192744\n",
      "423 0.00024656913592480123\n",
      "424 0.0002449277671985328\n",
      "425 0.00024435605155304074\n",
      "426 0.00024385174037888646\n",
      "427 0.00024314963957294822\n",
      "428 0.00024478809791617095\n",
      "429 0.0002449096064083278\n",
      "430 0.0002452536718919873\n",
      "431 0.00024315365590155125\n",
      "432 0.0002438665833324194\n",
      "433 0.00024326681159436703\n",
      "434 0.00024059928546193987\n",
      "435 0.00023811608843971044\n",
      "436 0.00023749726824462414\n",
      "437 0.000235153958783485\n",
      "438 0.0002326408284716308\n",
      "439 0.0002302098146174103\n",
      "440 0.00022652361076325178\n",
      "441 0.00022403898765332997\n",
      "442 0.00022323225857689977\n",
      "443 0.00022265978623181581\n",
      "444 0.00022302501020021737\n",
      "445 0.0002221641771029681\n",
      "446 0.00022219194215722382\n",
      "447 0.00022100406931713223\n",
      "448 0.000220439862459898\n",
      "449 0.0002201997849624604\n",
      "450 0.00021941559680271894\n",
      "451 0.0002180412266170606\n",
      "452 0.00021640583872795105\n",
      "453 0.00021534229745157063\n",
      "454 0.00021450110943987966\n",
      "455 0.00021362408006098121\n",
      "456 0.00021255880710668862\n",
      "457 0.00021122556063346565\n",
      "458 0.00021023672888986766\n",
      "459 0.00020944923744536936\n",
      "460 0.00020895159104838967\n",
      "461 0.00020770446280948818\n",
      "462 0.00020802949438802898\n",
      "463 0.00020531442714855075\n",
      "464 0.00020413496531546116\n",
      "465 0.00020120537374168634\n",
      "466 0.00020220996520947665\n",
      "467 0.0002017428632825613\n",
      "468 0.00020009656145703048\n",
      "469 0.0001975549675989896\n",
      "470 0.00019609529408626258\n",
      "471 0.00019593678007367998\n",
      "472 0.00019496565801091492\n",
      "473 0.00019439416064415127\n",
      "474 0.00019356404663994908\n",
      "475 0.00019360233272891492\n",
      "476 0.00019118146155960858\n",
      "477 0.0001895049208542332\n",
      "478 0.0001867763203335926\n",
      "479 0.00018638170149642974\n",
      "480 0.00018665434618014842\n",
      "481 0.0001859016774687916\n",
      "482 0.00018441183783579618\n",
      "483 0.00018471993098501116\n",
      "484 0.00018587993690744042\n",
      "485 0.00018558050214778632\n",
      "486 0.00018510564405005425\n",
      "487 0.00018488864589016885\n",
      "488 0.0001847657549660653\n",
      "489 0.00018336052016820759\n",
      "490 0.00018342332623433322\n",
      "491 0.0001852223213063553\n",
      "492 0.0001856778544606641\n",
      "493 0.0001846101658884436\n",
      "494 0.0001838956231949851\n",
      "495 0.0001839720644056797\n",
      "496 0.00018251911387778819\n",
      "497 0.00018277415074408054\n",
      "498 0.0001813276467146352\n",
      "499 0.00018029534840025008\n",
      "CPU times: user 1min 15s, sys: 7.72 s, total: 1min 23s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10000, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-8\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         raise RuntimeError(\n\u001b[1;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10000, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-8\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size) #define linear classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss() #criterion is variable that holds the function that should calculate the loss for me \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "#model.parameters() now the model knows what to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 3.7377\n",
      "Epoch [20/100], Loss: 0.7695\n",
      "Epoch [30/100], Loss: 0.2823\n",
      "Epoch [40/100], Loss: 0.2022\n",
      "Epoch [50/100], Loss: 0.1890\n",
      "Epoch [60/100], Loss: 0.1868\n",
      "Epoch [70/100], Loss: 0.1864\n",
      "Epoch [80/100], Loss: 0.1862\n",
      "Epoch [90/100], Loss: 0.1861\n",
      "Epoch [100/100], Loss: 0.1860\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train) #takes numpy array and outputs tensor \n",
    "    targets = torch.from_numpy(y_train) \n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)    #link to optimizer \n",
    "    #loss communicates with the parameter not the optimizer\n",
    "    \n",
    "    #batch gradient\n",
    "    # Backward and optimize\n",
    "    #Every time a variable is back propogated through, the gradient will be accumulated instead of being replaced so need to set to 0\n",
    "    optimizer.zero_grad()     #clear gradients each iteration before calculating loss \n",
    "    loss.backward()    #every tensor has back propogation value associated with parameters. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True. \n",
    "    #After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively\n",
    "    optimizer.step()     #performs a parameter update based on the current gradient\n",
    "    #step() method, that updates the parameters.  \n",
    "    #the update is not the based on the “closest” call but on the .grad attribute\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    #loss.item() gets the a scalar value held in the loss.\n",
    "\n",
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend() #displays legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
