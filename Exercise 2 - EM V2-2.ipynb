{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the population is given by GMM with 2 gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = -1\n",
    "sigma1 = 1\n",
    "x1 = np.linspace(mu1 - 3 * sigma1, mu1 + 3 * sigma1, 1000)\n",
    "y1 = norm.pdf(x1, mu1, sigma1)\n",
    "\n",
    "mu2 = 5\n",
    "sigma2 = 2\n",
    "x2 = np.linspace(mu2 - 3 * sigma2, mu2 + 3 * sigma2, 1000)\n",
    "y2 = norm.pdf(x2, mu2, sigma2)\n",
    "\n",
    "plt.plot(x1, y1, color='red', lw=2, ls='-', alpha=0.5)\n",
    "plt.plot(x2, y2, color='blue', lw=2, ls='-', alpha=0.5)\n",
    "\n",
    "plt.xlim(min(mu1 - 3 * sigma1, mu2 - 3 * sigma2), max(mu1 + 3 * sigma1, mu2 + 3 * sigma2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is a sample of 500 instances from each gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "first_gaussian = norm.rvs(loc=mu1, scale=sigma1, size=500)\n",
    "second_gaussian = norm.rvs(loc=mu2, scale=sigma2, size=500)\n",
    "xs = np.concatenate((first_gaussian, second_gaussian))\n",
    "df = pd.DataFrame(data={'x': xs})\n",
    "ax = df.plot.hist(bins=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample data we want to run EM in order to find the population distribution. <br>\n",
    "We first need to decide about the the distribution parameters. <br>\n",
    "After seeing the sample plot we can assume that the distribution of the population is GMM woth 2 gaussians (remember that we won't see the first plot of the real distribution, but only the sample plot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: init the GaussianMixture with 2 components\n",
    "from sklearn.mixture import GaussianMixture\n",
    "EM = \n",
    "# TODO: fit your model with the given data and predict the gaussian for each data point \n",
    "# hint: use sklearn.mixture.GaussianMixture ducomentation if you need help with find the relevant method\n",
    "z = \n",
    "\n",
    "df['z'] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for ploting the original distribution with the one that we found with the EM\n",
    "plt.figure(figsize=(10, 7))\n",
    "mu_hat1 = df['x'][df['z'] == 0].mean()\n",
    "sigma_hat1 = df['x'][df['z'] == 0].std()\n",
    "x_hat1 = np.linspace(mu_hat1 - 3 * sigma_hat1, mu_hat1 + 3 * sigma_hat1, 1000)\n",
    "y_hat1 = norm.pdf(x_hat1, mu_hat1, sigma_hat1)\n",
    "\n",
    "mu_hat2 = df['x'][df['z'] == 1].mean()\n",
    "sigma_hat2 = df['x'][df['z'] == 1].std()\n",
    "x_hat2 = np.linspace(mu_hat2 - 3 * sigma_hat2, mu_hat2 + 3 * sigma_hat2, 1000)\n",
    "y_hat2 = norm.pdf(x_hat2, mu_hat2, sigma_hat2)\n",
    "\n",
    "plt.plot(x_hat1, y_hat1, color='red', lw=1, ls='-', alpha=0.5)\n",
    "plt.plot(x_hat2, y_hat2, color='blue', lw=1, ls='-', alpha=0.5)\n",
    "\n",
    "plt.xlim(min(mu_hat1 - 3 * sigma_hat1, mu_hat2 - 3 * sigma_hat2), \n",
    "         max(mu_hat1 + 3 * sigma_hat1, mu_hat2 + 3 * sigma_hat2))\n",
    "\n",
    "mu1 = -1\n",
    "sigma1 = 1\n",
    "x1 = np.linspace(mu1 - 3 * sigma1, mu1 + 3 * sigma1, 1000)\n",
    "y1 = norm.pdf(x1, mu1, sigma1)\n",
    "\n",
    "mu2 = 5\n",
    "sigma2 = 2\n",
    "x2 = np.linspace(mu2 - 3 * sigma2, mu2 + 3 * sigma2, 1000)\n",
    "y2 = norm.pdf(x2, mu2, sigma2)\n",
    "\n",
    "plt.plot(x1, y1, color='red', lw=1, ls='--', alpha=0.5)\n",
    "plt.plot(x2, y2, color='blue', lw=1, ls='--', alpha=0.5)\n",
    "\n",
    "plt.xlim(min(mu1 - 3 * sigma1, mu2 - 3 * sigma2), max(mu1 + 3 * sigma1, mu2 + 3 * sigma2))\n",
    "\n",
    "plt.legend(['Predicted - 1st gaussian', 'Predicted - 2nd gaussian', \n",
    "            'Original - 1st gaussian', 'Original - 2nd gaussian'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(mu_hat1, sigma_hat1, mu1, sigma1)\n",
    "print(mu_hat2, sigma_hat2, mu2, sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we almost found the original parameter only with 500 samples from each gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will implement the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df = pd.read_csv('data.csv')\n",
    "# TODO: plot the data in order to decide about the parameters of the distribution, use 70 bins\n",
    "ax = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: given the above plot, decide from how many guassian you think the data came from\n",
    "K = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented for you the EM wrraper algorithm.<br>\n",
    "This function calls to helper functions that you need to implement:<br>\n",
    "1. init - gussing the initial values of W, Mu, sigma\n",
    "2. Expectation - perform the E-step\n",
    "3. Maximization - perform the M-step\n",
    "4. calc_delta - calculating the diffrence between the old parms to the new params for the stopping condition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(data, K):\n",
    "    \"\"\"\n",
    "    :param data: the complete data\n",
    "    :param K: number of gaussians\n",
    "    :return the initial guess of W, Mu, sigma\n",
    "    \"\"\"\n",
    "    # TODO: complete the init function\n",
    "    # Make initial guess\n",
    "    N = data.shape[0]\n",
    "    # guess each w as 1/K\n",
    "    W = \n",
    "\n",
    "    # Guess mu as center of K quantiles, see pandas.qcut documentation\n",
    "    quat_data, bins = \n",
    "    Mu = [0] * K\n",
    "    for k in range(K):\n",
    "        Mu[k] = \n",
    "\n",
    "    # Guess sigma as empiric sigma\n",
    "    sum = [0] * K\n",
    "    for index, bin in quat_data.iteritems():\n",
    "        sum[bin] += \n",
    "    sigma = [0] * K\n",
    "    for k in range(K):\n",
    "        sigma[k] = \n",
    "    return W, Mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expectation(data, Mu, sigma, W):\n",
    "    \"\"\"\n",
    "    :param data: the complete data\n",
    "    :param Mu: expectation of each gaussian\n",
    "    :param sigma: std for of gaussian\n",
    "    :param W: weight of each gaussian\n",
    "    :return ranks matrix: r(x,k)- responsibility of each data point x to gaussian k \n",
    "            Q: dividend of ranks matrix (likelihood)\n",
    "    \"\"\"\n",
    "    K = W.__len__()\n",
    "    rank_df = pd.DataFrame(columns=range(K))\n",
    "    Q_df = pd.DataFrame(columns=range(K))\n",
    "\n",
    "    return rank_df, Q_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Maximization(data, K, ranks):\n",
    "    \"\"\"\n",
    "    :param data: the complete data\n",
    "    :param K: number of gaussians\n",
    "    :param ranks: ranks matrix- r(x,k)- responsibility of each data point x to gaussian k \n",
    "    :return W_new: new weight parameter of each gaussian \n",
    "            Mu_new: new expectation parameter of each gaussian \n",
    "            sigma_new: new std parameter of each gaussian \n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    ranks_sum = ranks.sum(axis=0).values\n",
    "    W_new = [0] * K\n",
    "    Mu_new = [0] * K\n",
    "    sigma_new = [0] * K\n",
    "\n",
    "\n",
    "    return W_new, Mu_new, sigma_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta(old_param, new_param):\n",
    "    \"\"\"\n",
    "    :param old_param: old parameters to compare\n",
    "    :param new_param: new parameters to compare\n",
    "    :return maximal delta between each old and new parameter\n",
    "    \"\"\"\n",
    "    K = old_param.__len__()\n",
    "    max_delta = 0\n",
    "    # TODO: find the maximal delta between each old and new parameter\n",
    "    for k in range(K):\n",
    "        \n",
    "    return max_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Already implemented functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting\n",
    "def plotGMM(K, res, Mu, sigma, data, iter=-1):\n",
    "    for k in range(K):\n",
    "        res_bin= res[res == k]\n",
    "        dots = data[\"x\"][res_bin.index]\n",
    "        plt.scatter(dots.values, norm.pdf(dots.values, loc=Mu[k], scale=sigma[k]), \n",
    "                    label=\"Mu=%.2f, Sigma=%.2f\"%(Mu[k], sigma[k]), s=10)\n",
    "    plt.ylabel('probability')\n",
    "    if iter>=0:\n",
    "        plt.title('Expectation Maximization - GMM - iteration {}'.format(iter))\n",
    "    else:\n",
    "        plt.title('Expectation Maximization - GMM')\n",
    "    plt.legend()\n",
    "    plt.ylim(0,0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpectationMaximization(data, K, maxIter, epsilon):\n",
    "    \"\"\"\n",
    "    :param data: the complete data\n",
    "    :param K: number of gaussians\n",
    "    :param maxIter: maximal number of iterations to perform\n",
    "    :param epsilon: minimal change in parameters to declare convergence\n",
    "    :return gaussian estimation for each point\n",
    "    \"\"\"\n",
    "\n",
    "    W, Mu, sigma = init(data, K)\n",
    "\n",
    "    # Loop until convergence\n",
    "    delta = np.infty\n",
    "    iter = 0\n",
    "    \n",
    "    log_likelihood = []\n",
    "    while delta > epsilon and iter <= maxIter:\n",
    "        # E step\n",
    "        ranks, Q = Expectation(data, Mu, sigma, W)\n",
    "        \n",
    "        # ranks form the responsibilities matrix\n",
    "        likelihood = Q.sum(axis=1)\n",
    "        log_likelihood.append(np.sum(np.log(likelihood.values), axis = 0))\n",
    "\n",
    "        # M step\n",
    "        W_new, Mu_new, sigma_new = Maximization(data, K, ranks)\n",
    "\n",
    "        # Check significant change in parameters\n",
    "        delta = max(calc_delta(W, W_new), calc_delta(Mu, Mu_new), calc_delta(sigma, sigma_new))\n",
    "        W, Mu, sigma = W_new, Mu_new, sigma_new\n",
    "\n",
    "        if iter%10 == 0:\n",
    "            res = ranks.idxmax(axis=1)\n",
    "            plotGMM(K, res, Mu, sigma, data, iter)\n",
    "        iter += 1\n",
    "\n",
    "    print()\n",
    "    plt.show()\n",
    "    \n",
    "    res = ranks.idxmax(axis=1)\n",
    "\n",
    "    # Display estimated Gaussian:\n",
    "    plotGMM(K, res, Mu, sigma, data, iter)\n",
    "\n",
    "    # Display log likelihood changes:\n",
    "    plt.plot(log_likelihood)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('log likelihood')\n",
    "    plt.title('Log likelihood over iteration')\n",
    "    plt.show()\n",
    "    \n",
    "    return res, Mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the EM algorithm\n",
    "res, Mu, sigma = ExpectationMaximization(df, K, 1000, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the output distribution \n",
    "plotGMM(K, res, Mu, sigma, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
