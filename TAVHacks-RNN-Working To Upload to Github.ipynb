{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms \n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Image Captioning in PyTorch\n",
    "\n",
    "The goal of image captioning is to describe a given image using natural language. Using neural networks, the problem can be partitioned into two separate challenges. First, we need to extract meaningful features regarding the image that would help us describe it. Second, we need to generate a sequence of words that best fit those features. The flexability of neural networks allows us to take a CNN architecture and connect it directly to a LSTM network. We only need to provide proper labels to train the new network we created. \n",
    "We will use pretrained networks for both feature extraction and sentence generation, and we will connect the different components needed to make image captioning work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define an encoder and decoder networks seperately. The encoder takes an image and produces a latent vector of features. As those features hold information about the image, we will use that vector as the input for our decoder. The RNN decoder will produce the image captioning using an LSTM architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the pretrained [resnet152](https://www.dropbox.com/s/ne0ixz5d58ccbbz/pretrained_model.zip?dl=0) and [vocabulary](https://www.dropbox.com/s/26adb7y9m98uisa/vocap.zip?dl=0) and place them in the following folder hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slogans_path  ✗\n",
      "encoder_path  ✗\n",
      "decoder_path  ✗\n",
      "vocab_path  ✗\n"
     ]
    }
   ],
   "source": [
    "slogans_path = 'SlogansAndTypes.csv'\n",
    "encoder_path = 'pretrained_model/encoder-5-3000.pkl'\n",
    "decoder_path = 'pretrained_model/decoder-5-3000.pkl'\n",
    "vocab_path   = 'vocab.pkl'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "# check if all the files are in place\n",
    "print(\"slogans_path \", '✓' if os.path.isfile(slogans_path) == True else '✗') \n",
    "print(\"encoder_path \", '✓' if os.path.isfile(encoder_path) == True else '✗') \n",
    "print(\"decoder_path \", '✓' if os.path.isfile(decoder_path) == True else '✗') \n",
    "print(\"vocab_path \", '✓' if os.path.isfile(vocab_path) == True else '✗') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size   = 256      # dimension of word embedding vectors\n",
    "hidden_size  = 512      # dimension of lstm hidden states\n",
    "num_layers   = 1        # number of layers in lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the architectures\n",
    "\n",
    "When loading pretrained networks in PyTorch, a common practice is to download the weights. In this case, you are required to define the same architecture in order to use a simple function that places the parameters in the correct place in the network. However, PyTorch has built-in functions to load the **architectures** of the networks we will be using. Later on, we will use the files we downloaded to load the initialized architectures with pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        #############################################################################\n",
    "        # Load the pretrained ResNet-152 network and replace the top fully          #\n",
    "        # connected layer, so we could pass the features of the network and not the #\n",
    "        # classification which carries significantly less information.              #\n",
    "        # Afterwards, create a new sequential model which includes the resnet and   #\n",
    "        # add a new fully connected layer that outputs a vector with the size of    #\n",
    "        # the wanted embedding. Next, add a batchnorm layer                         #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "        resnet = models.resnet152()\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "       \n",
    "    def forward(self, images):\n",
    "        #############################################################################\n",
    "        # Define the forward propagation. You need to pass an image through the     #\n",
    "        # network and extract the feature vector. In this case, when using a        #\n",
    "        # perdefined network, you don't want to change it's weights.                #\n",
    "        # The rest of the layers you defined should accepts gradients for them to   #\n",
    "        # improve during training. Make sure you are inputing a correct shape       #\n",
    "        # to the batchnorm layer.                                                   #\n",
    "        # This function returns the features of the image as a vector               #\n",
    "        #############################################################################\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.max_seg_length = max_seq_length\n",
    "        #############################################################################\n",
    "        # Define the hyper-parameters and the layers of the pretrained LSTM.        #\n",
    "        # Create an Embedding layer that accepts the output of the                  #\n",
    "        # feature extractor.  Next, the built-in LSTM architecture in PyTorch.nn    #\n",
    "        # with the proper inputs (use the built-in documentation tool in Jupyter    #\n",
    "        # or just look at the official documentation online).                       #\n",
    "        # Define an additional linear layer that comes after the LSTM and outputs   #\n",
    "        # a vector that will support the size of our vocabulary.                    #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        #############################################################################\n",
    "        # Decode image feature vectors and generate captions.                       #\n",
    "        # Since we do not need to train the network, you don't need to write the    # \n",
    "        # forward propagation.                                                      #\n",
    "        #############################################################################\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens = None\n",
    "        outputs = None\n",
    "        return outputs    \n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        #############################################################################\n",
    "        # Generate captions for a given image features.                             #\n",
    "        # First, obtain the output of the LSTM network (How many are they?).        #\n",
    "        # Next, use the hidden states to obtain the most probable word and store    #\n",
    "        # all the word predictions in the sampled_ids list. Don't forget to update  #\n",
    "        # the inputs for each timestep to continue making predictions based on the  #\n",
    "        # words you are alreaedy predicted.                                         #\n",
    "        # Make sure you keep track of the dimensions of the inputs and outputs,     #\n",
    "        # since PyTorch expects tensors with a batch dimension. You can use the     #\n",
    "        # methods .squeeze() and .unsqueeze()                                       #\n",
    "        # This function returns the list of predicted words.                        #\n",
    "        #############################################################################\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)       \n",
    "\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(slogan_path, transform=None):\n",
    "    slogans = pd.read_csv(slogan_path)\n",
    "    return slogans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vocab.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-59bb9deb13b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load vocabulary wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab.pkl'"
     ]
    }
   ],
   "source": [
    "# Load vocabulary wrapper\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-91fa0242c94c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# eval mode (batchnorm uses moving mean/variance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# Build models\n",
    "encoder = EncoderCNN(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Load the trained model parameters\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "decoder.load_state_dict(torch.load(decoder_path))\n",
    "\n",
    "# Prepare an image\n",
    "image = load_text(slogans_path, transform)\n",
    "image_tensor = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an caption from the image\n",
    "feature = encoder(image_tensor)\n",
    "sampled_ids = decoder.sample(feature)\n",
    "sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "# Convert word_ids to words\n",
    "sampled_caption = []\n",
    "for word_id in sampled_ids:\n",
    "    word = vocab.idx2word[word_id]\n",
    "    sampled_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "sentence = ' '.join(sampled_caption)\n",
    "\n",
    "# Print out the image and the generated caption\n",
    "print(sentence)\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(np.asarray(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
